"""
LangGraph Nodes êµ¬í˜„

ê° NodeëŠ” Stateë¥¼ ì…ë ¥ë°›ì•„ ì²˜ë¦¬ í›„ Stateë¥¼ ë°˜í™˜
"""

import logging
from time import time
from typing import Dict

from langchain_core.messages import AIMessage, HumanMessage

from app.core.context_builder import build_context
from app.core.embedding import get_query_embedding
from app.core.graph_state import ConversationState
from app.core.llm import generate_follow_up_questions, llm
from app.core.mretriever import retrieve_chunks_for_document, should_use_chunks
from app.core.prompts import (
    UserLevel,
    get_conversation_prompt,
)


logger = logging.getLogger(__name__)


# ===================================
# Node 1: RAG Retrieval
# ===================================


async def rag_retrieve_node(state: ConversationState) -> Dict:
    """
    RAG ê²€ìƒ‰ ë…¸ë“œ

    1. ì§ˆë¬¸ ì„ë² ë”© ìƒì„±
    2. Vector ê²€ìƒ‰
    3. ì²­í¬ ì‚¬ìš© ì—¬ë¶€ íŒë‹¨
    4. ì»¨í…ìŠ¤íŠ¸ ì¡°í•©

    Returns:
        State ì—…ë°ì´íŠ¸ (query_embedding, retrieved_chunks, context, use_chunks)
    """
    start = time()

    question = state["question"]
    document_id = state.get("document_id")

    # 1. Embedding ìƒì„±
    query_embedding = get_query_embedding(question)

    # 2. Vector ê²€ìƒ‰ (ì„ì‹œë¡œ None ì²´í¬)
    if document_id:
        from app.database import SessionLocal

        db = SessionLocal()
        try:
            chunks = retrieve_chunks_for_document(
                db=db,
                embedding=query_embedding,
                document_id=document_id,
                top_k=3,
            )
        finally:
            db.close()
    else:
        chunks = []

    # 3. ì²­í¬ ì‚¬ìš© ì—¬ë¶€ íŒë‹¨
    decision = should_use_chunks(
        document_id=str(document_id) if document_id else None,
        chunks=chunks,
        similarity_threshold=0.7,
    )

    # 4. ì»¨í…ìŠ¤íŠ¸ ì¡°í•©
    context = build_context(chunks) if decision["use_chunks"] else ""

    elapsed = int((time() - start) * 1000)
    logger.info(
        f"ğŸ” RAG Retrieve: {elapsed}ms, chunks={len(chunks)}, use={decision['use_chunks']}"
    )

    return {
        "query_embedding": query_embedding,
        "retrieved_chunks": [
            {
                "id": str(chunk.id),
                "content": chunk.content[:200],
                "similarity": chunk.similarity if hasattr(chunk, "similarity") else 0,
            }
            for chunk in chunks
        ],
        "context": context,
        "use_chunks": decision["use_chunks"],
        "decision_reason": decision["reason"],
        "max_similarity": decision["max_similarity"],
    }


# ===================================
# Node 2: LLM Generation
# ===================================


async def llm_generate_node(state: ConversationState) -> Dict:
    """
    LLM ë‹µë³€ ìƒì„± ë…¸ë“œ

    1. í”„ë¡¬í”„íŠ¸ ì„ íƒ
    2. ì‚¬ìš©ì ë©”ì‹œì§€ êµ¬ì„±
    3. LLM ì²´ì¸ ì‹¤í–‰
    4. ë©”ì‹œì§€ ìƒíƒœ ì—…ë°ì´íŠ¸

    Returns:
        State ì—…ë°ì´íŠ¸ (answer, messages, model_version, token_usage)
    """
    start = time()

    question = state["question"]
    context = state.get("context", "")
    document_id = state.get("document_id")
    user_level = state.get("user_level", "beginner")
    messages = state.get("messages", [])

    # 1. UserLevel enum ë³€í™˜
    try:
        level = UserLevel(user_level.lower())
    except ValueError:
        level = UserLevel.BEGINNER

    # 2. ì‹œë‚˜ë¦¬ì˜¤ì— ë§ëŠ” í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì„ íƒ
    prompt = get_conversation_prompt(
        user_level=level,
        document_id=str(document_id) if document_id else None,
        context_exists=bool(context and context.strip()),
    )

    # 3. LLM ì²´ì¸ êµ¬ì„±
    chain = prompt | llm

    # 4. ì‚¬ìš©ì ë©”ì‹œì§€ êµ¬ì„± (ì»¨í…ìŠ¤íŠ¸ í¬í•¨)
    if context and context.strip():
        user_message_content = f"""[ê²€ìƒ‰ëœ ë¬¸ì„œ ì •ë³´]
{context}

[í˜„ì¬ ì§ˆë¬¸]
{question}"""
    else:
        user_message_content = question

    # 5. LLM í˜¸ì¶œ
    result = await chain.ainvoke(
        {"messages": messages + [HumanMessage(content=user_message_content)]}
    )
    answer = result.content.strip()

    # 6. í† í° ì‚¬ìš©ëŸ‰ ì¶”ì¶œ
    token_usage = {}
    if hasattr(result, "response_metadata"):
        metadata = result.response_metadata
        token_usage = {
            "prompt": metadata.get("prompt_tokens", 0),
            "completion": metadata.get("completion_tokens", 0),
            "total": metadata.get("total_tokens", 0),
        }

    # 7. ë©”ì‹œì§€ ì—…ë°ì´íŠ¸ (Human + AI)
    # HumanMessageëŠ” ì»¨í…ìŠ¤íŠ¸ ì—†ì´ ì›ë³¸ ì§ˆë¬¸ë§Œ ì €ì¥í•˜ì—¬ íˆìŠ¤í† ë¦¬ UIì— í‘œì‹œ
    updated_messages = messages + [
        HumanMessage(content=question),
        AIMessage(content=answer),
    ]

    elapsed = int((time() - start) * 1000)
    logger.info(f"ğŸ¤– LLM Generate: {elapsed}ms, tokens={token_usage.get('total', 0)}")

    return {
        "answer": answer,
        "messages": updated_messages,
        "model_version": "solar-pro2",
        "token_usage": token_usage,
    }


# ===================================
# Node 3: Follow-up Questions
# ===================================


async def followup_node(state: ConversationState) -> Dict:
    """
    í›„ì† ì§ˆë¬¸ ìƒì„± ë…¸ë“œ

    Returns:
        State ì—…ë°ì´íŠ¸ (follow_up_questions)
    """
    start = time()

    question = state["question"]
    answer = state["answer"]
    context = state.get("context", "")
    user_level = state.get("user_level", "beginner")

    # UserLevel enum ë³€í™˜
    try:
        level = UserLevel(user_level.lower())
    except ValueError:
        level = UserLevel.BEGINNER

    # í›„ì† ì§ˆë¬¸ ìƒì„±
    follow_ups = generate_follow_up_questions(
        question=question,
        answer=answer,
        context=context,
        user_level=level,
        num_questions=3,
    )

    elapsed = int((time() - start) * 1000)
    logger.info(f"ğŸ’¡ Followup: {elapsed}ms, count={len(follow_ups)}")

    return {
        "follow_up_questions": follow_ups,
    }
